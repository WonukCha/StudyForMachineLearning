{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    r = 1/(1+np.exp(-x))\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    dx = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    #print (\"ng. initial input x = {}\".format(x))\n",
    "    #print (\"ng. initial grad x = {}\".format(grad))\n",
    "    #print (\"============================================\")\n",
    "    \n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        #print(\"ng, index = {}\".format(idx))\n",
    "        temp = x[idx]    #numpy 타입은 mutable 이므러 원래 값 보관 하고 연산 후 복원 필요\n",
    "        #f(x+dx) 계산\n",
    "        x[idx] = float(temp) +dx\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        #f(x-dx) 계산\n",
    "        x[idx] = float(temp) -dx\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1-fx2)/(2*dx)\n",
    "        x[idx] =temp #값 복원\n",
    "        it.iternext()\n",
    "       # print('ng. result grad[idx] = {}'.format(grad[idx]))\n",
    "       # print('ng. result grad= {}'.format(grad))\n",
    "       # print (\"============================================\")\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleLogicGate:\n",
    "    def __init__(self,gate,xData,tData):\n",
    "        self.gate = gate\n",
    "        #입력 및 정답 데이터 초기화\n",
    "        self.__xData = xData.reshape(4,2) # 입력 데이터 4 by 2 행렬로 형상 변환\n",
    "        self.__tData = tData.reshape(4,1) # 정답 데이터 4 by 1 행렬로 형상 변환\n",
    "        # 가중치 W 바이어스 b 초기화\n",
    "        self.__W = np.random.rand(2,1) # 입력 데이터 변수가 2개 이브로 2by1 행렬로\n",
    "        self.__b = np.random.rand(1)\n",
    "        #학습을 위한 learning rate 초기화\n",
    "        self.__learningRate = 1e-2\n",
    "        \n",
    "    def __loss(self):\n",
    "        dx = 1e-7\n",
    "        z = np.dot(self.__xData, self.__W)+self.__b\n",
    "        y = sigmoid(z)\n",
    "        #cross-entropy\n",
    "        return -np.sum(self.__tData*np.log(y+dx)+(1-self.__tData)*np.log((1-y)+dx))\n",
    "    \n",
    "    def __print_loss(self):\n",
    "        print(\"loss value = {}, W = {}, b = {}\".format(self.__loss(),self.__W,self.__b))\n",
    "        \n",
    "    def training(self):\n",
    "        f = lambda x :self.__loss()\n",
    "        print(\"initial\",end=\" \")\n",
    "        self.__print_loss()\n",
    "        for step in range(8001):\n",
    "            self.__W -= self.__learningRate + numerical_gradient(f,self.__W)\n",
    "            self.__b -= self.__learningRate + numerical_gradient(f,self.__b)\n",
    "            if(step % 400 ==0):\n",
    "                print(\"step = {}\".format(step),end=\" \")\n",
    "                self.__print_loss()\n",
    "                \n",
    "    def predict(self,x):\n",
    "        z = np.dot(x,self.__W)+self.__b\n",
    "        y = sigmoid(z)\n",
    "        if(y > 0.5):\n",
    "            result =1\n",
    "        else:\n",
    "            result = 0\n",
    "        return y,result            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    y = np.dot(x,W)+b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss value = 3.671740318207162, W = [[0.77924149]\n",
      " [0.56001114]], b = [0.32952518]\n",
      "step = 0 loss value = 2.1679725371015275, W = [[ 0.17592956]\n",
      " [-0.00019616]], b = [-1.0911495]\n",
      "step = 400 loss value = 0.04642553469199133, W = [[8.20799268]\n",
      " [8.20799256]], b = [-12.74311556]\n",
      "step = 800 loss value = 0.027466138993009405, W = [[9.32355572]\n",
      " [9.32355571]], b = [-14.59264085]\n",
      "step = 1200 loss value = 0.021209641678562305, W = [[9.93094833]\n",
      " [9.93094833]], b = [-15.64318493]\n",
      "step = 1600 loss value = 0.01815863927810226, W = [[10.33940182]\n",
      " [10.33940182]], b = [-16.36961457]\n",
      "step = 2000 loss value = 0.01637459229078978, W = [[10.64358482]\n",
      " [10.64358482]], b = [-16.92125558]\n",
      "step = 2400 loss value = 0.015213068396278662, W = [[10.88434567]\n",
      " [10.88434567]], b = [-17.36415462]\n",
      "step = 2800 loss value = 0.014400744082164865, W = [[11.08278821]\n",
      " [11.08278821]], b = [-17.73317763]\n",
      "step = 3200 loss value = 0.013802773860533057, W = [[11.25113878]\n",
      " [11.25113878]], b = [-18.04889877]\n",
      "step = 3600 loss value = 0.013345308437409609, W = [[11.39707215]\n",
      " [11.39707215]], b = [-18.32443617]\n",
      "step = 4000 loss value = 0.012984667526888713, W = [[11.52570178]\n",
      " [11.52570178]], b = [-18.56864707]\n",
      "step = 4400 loss value = 0.012693436644680266, W = [[11.6405954]\n",
      " [11.6405954]], b = [-18.78778208]\n",
      "step = 4800 loss value = 0.012453578644395065, W = [[11.7443355]\n",
      " [11.7443355]], b = [-18.98641048]\n",
      "step = 5200 loss value = 0.012252760519266662, W = [[11.83884841]\n",
      " [11.83884841]], b = [-19.16796984]\n",
      "step = 5600 loss value = 0.012082275232579832, W = [[11.92560735]\n",
      " [11.92560735]], b = [-19.33510897]\n",
      "step = 6000 loss value = 0.011935807125440491, W = [[12.00576307]\n",
      " [12.00576307]], b = [-19.48991056]\n",
      "step = 6400 loss value = 0.011808667771643376, W = [[12.08023079]\n",
      " [12.08023079]], b = [-19.63404081]\n",
      "step = 6800 loss value = 0.011697306185781808, W = [[12.14974987]\n",
      " [12.14974987]], b = [-19.76885284]\n",
      "step = 7200 loss value = 0.011598985265341486, W = [[12.21492586]\n",
      " [12.21492586]], b = [-19.89546011]\n",
      "step = 7600 loss value = 0.011511562342777358, W = [[12.2762607]\n",
      " [12.2762607]], b = [-20.01478945]\n",
      "step = 8000 loss value = 0.011433336864256195, W = [[12.33417491]\n",
      " [12.33417491]], b = [-20.12762039]\n"
     ]
    }
   ],
   "source": [
    "#And Gate 학습\n",
    "xData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tData = np.array([0,0,0,1])\n",
    "\n",
    "andGate = SimpleLogicGate(\"AND Gate\",xData,tData)\n",
    "andGate.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AND Gate 학습 결과 검증\n",
    "print(andGate.gate,\"\\n\")\n",
    "testData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for x in testData:\n",
    "    (y,predicted) = andGate.predict(x)\n",
    "    print(x, \" = \",predicted,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss value = 1.7440884104787866, W = [[0.4723613 ]\n",
      " [0.86651594]], b = [0.96300817]\n",
      "step = 0 loss value = 1.5145911991758805, W = [[0.74559164]\n",
      " [1.08577782]], b = [0.55443372]\n",
      "step = 400 loss value = 0.04197306197645652, W = [[8.44727283]\n",
      " [8.4472803 ]], b = [-4.07916955]\n",
      "step = 800 loss value = 0.033105033278469184, W = [[8.97251872]\n",
      " [8.9725188 ]], b = [-4.43845423]\n",
      "step = 1200 loss value = 0.031074917747334944, W = [[9.11902346]\n",
      " [9.11902346]], b = [-4.54330905]\n",
      "step = 1600 loss value = 0.030457128913570785, W = [[9.16629207]\n",
      " [9.16629207]], b = [-4.57760692]\n",
      "step = 2000 loss value = 0.030253244698365316, W = [[9.18219488]\n",
      " [9.18219488]], b = [-4.58919815]\n",
      "step = 2400 loss value = 0.030184162485831677, W = [[9.18761847]\n",
      " [9.18761847]], b = [-4.59315734]\n",
      "step = 2800 loss value = 0.030160546524193768, W = [[9.18947667]\n",
      " [9.18947667]], b = [-4.59451453]\n",
      "step = 3200 loss value = 0.03015244883116632, W = [[9.19011432]\n",
      " [9.19011432]], b = [-4.59498034]\n",
      "step = 3600 loss value = 0.030149669322248725, W = [[9.19033325]\n",
      " [9.19033325]], b = [-4.59514027]\n",
      "step = 4000 loss value = 0.03014871492399532, W = [[9.19040843]\n",
      " [9.19040843]], b = [-4.5951952]\n",
      "step = 4400 loss value = 0.030148387172074857, W = [[9.19043425]\n",
      " [9.19043425]], b = [-4.59521406]\n",
      "step = 4800 loss value = 0.03014827461441893, W = [[9.19044311]\n",
      " [9.19044311]], b = [-4.59522054]\n",
      "step = 5200 loss value = 0.03014823595847408, W = [[9.19044616]\n",
      " [9.19044616]], b = [-4.59522276]\n",
      "step = 5600 loss value = 0.030148222683050188, W = [[9.1904472]\n",
      " [9.1904472]], b = [-4.59522353]\n",
      "step = 6000 loss value = 0.030148218123620618, W = [[9.19044756]\n",
      " [9.19044756]], b = [-4.59522379]\n",
      "step = 6400 loss value = 0.030148216557561957, W = [[9.19044769]\n",
      " [9.19044769]], b = [-4.59522388]\n",
      "step = 6800 loss value = 0.030148216019861007, W = [[9.19044773]\n",
      " [9.19044773]], b = [-4.59522391]\n",
      "step = 7200 loss value = 0.03014821583526495, W = [[9.19044774]\n",
      " [9.19044774]], b = [-4.59522392]\n",
      "step = 7600 loss value = 0.030148215772060543, W = [[9.19044775]\n",
      " [9.19044775]], b = [-4.59522392]\n",
      "step = 8000 loss value = 0.030148215750660307, W = [[9.19044775]\n",
      " [9.19044775]], b = [-4.59522393]\n"
     ]
    }
   ],
   "source": [
    "#OR Gate 학습\n",
    "xData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tData = np.array([0,1,1,1])\n",
    "\n",
    "orGate = SimpleLogicGate(\"OR Gate\", xData,tData)\n",
    "orGate.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Gate \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OR Gate 학습 결과 검증\n",
    "print(orGate.gate,\"\\n\")\n",
    "testData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for x in testData:\n",
    "    (y,predicted) = orGate.predict(x)\n",
    "    print(x, \" = \",predicted,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss value = 2.836654257242043, W = [[0.04124054]\n",
      " [0.78851626]], b = [0.18317629]\n",
      "step = 0 loss value = 2.300837410749593, W = [[-0.25822295]\n",
      " [ 0.319466  ]], b = [0.96238007]\n",
      "step = 400 loss value = 0.036920689342936255, W = [[-8.92987547]\n",
      " [-8.92987535]], b = [13.21514863]\n",
      "step = 800 loss value = 0.019382033541655023, W = [[-10.79792565]\n",
      " [-10.79792565]], b = [15.56202197]\n",
      "step = 1200 loss value = 0.015028020953941634, W = [[-12.26320318]\n",
      " [-12.26320318]], b = [17.19509686]\n",
      "step = 1600 loss value = 0.013802182384817743, W = [[-13.63400102]\n",
      " [-13.63400102]], b = [18.61942587]\n",
      "step = 2000 loss value = 0.013468140945830407, W = [[-14.9813169]\n",
      " [-14.9813169]], b = [19.9819477]\n",
      "step = 2400 loss value = 0.013379451138388437, W = [[-16.32265701]\n",
      " [-16.32265701]], b = [21.32737441]\n",
      "step = 2800 loss value = 0.013356124108497062, W = [[-17.66244788]\n",
      " [-17.66244788]], b = [22.66824368]\n",
      "step = 3200 loss value = 0.01335000532461538, W = [[-19.00183404]\n",
      " [-19.00183404]], b = [24.00791296]\n",
      "step = 3600 loss value = 0.013348401514029308, W = [[-20.34111424]\n",
      " [-20.34111424]], b = [25.34726738]\n",
      "step = 4000 loss value = 0.013347981216061412, W = [[-21.68036668]\n",
      " [-21.68036668]], b = [26.68653927]\n",
      "step = 4400 loss value = 0.01334787107744046, W = [[-23.01961184]\n",
      " [-23.01961184]], b = [28.02578954]\n",
      "step = 4800 loss value = 0.013347842216137846, W = [[-24.3588551]\n",
      " [-24.3588551]], b = [29.36503413]\n",
      "step = 5200 loss value = 0.013347834653249308, W = [[-25.69809786]\n",
      " [-25.69809786]], b = [30.70427724]\n",
      "step = 5600 loss value = 0.01334783267135089, W = [[-27.03734049]\n",
      " [-27.03734049]], b = [32.04351996]\n",
      "step = 6000 loss value = 0.013347832151819186, W = [[-28.37658308]\n",
      " [-28.37658308]], b = [33.38276258]\n",
      "step = 6400 loss value = 0.013347832015487988, W = [[-29.71582567]\n",
      " [-29.71582567]], b = [34.72200517]\n",
      "step = 6800 loss value = 0.013347831980077466, W = [[-31.05506825]\n",
      " [-31.05506825]], b = [36.06124776]\n",
      "step = 7200 loss value = 0.013347831970595164, W = [[-32.39431084]\n",
      " [-32.39431084]], b = [37.40049034]\n",
      "step = 7600 loss value = 0.013347831967970722, W = [[-33.73355342]\n",
      " [-33.73355342]], b = [38.73973292]\n",
      "step = 8000 loss value = 0.013347831967590693, W = [[-35.072796]\n",
      " [-35.072796]], b = [40.07897551]\n"
     ]
    }
   ],
   "source": [
    "#NAND Gate 학습\n",
    "xData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tData = np.array([1,1,1,0])\n",
    "nandGate = SimpleLogicGate(\"NAND Gate\",xData,tData)\n",
    "nandGate.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND Gate \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nandGate.gate,\"\\n\")\n",
    "testData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for x in testData:\n",
    "    (y, predicted) = nandGate.predict(x)\n",
    "    print(x,\" = \",predicted,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss value = 3.089793432045396, W = [[0.26750718]\n",
      " [0.10993985]], b = [0.60657357]\n",
      "step = 0 loss value = 2.787666720345696, W = [[-0.17599223]\n",
      " [-0.29980425]], b = [0.23465791]\n",
      "step = 400 loss value = 2.772737935376603, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 800 loss value = 2.7727379353766386, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 1200 loss value = 2.772737935376585, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 1600 loss value = 2.7727379353766426, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 2000 loss value = 2.772737935376656, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 2400 loss value = 2.7727379353766914, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 2800 loss value = 2.77273793537666, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 3200 loss value = 2.7727379353765844, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 3600 loss value = 2.772737935376575, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 4000 loss value = 2.772737935376633, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 4400 loss value = 2.7727379353765795, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 4800 loss value = 2.772737935376593, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 5200 loss value = 2.772737935376629, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 5600 loss value = 2.7727379353766413, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 6000 loss value = 2.7727379353766106, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 6400 loss value = 2.7727379353766684, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 6800 loss value = 2.772737935376659, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 7200 loss value = 2.772737935376628, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 7600 loss value = 2.772737935376619, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n",
      "step = 8000 loss value = 2.77273793537661, W = [[-0.02000117]\n",
      " [-0.02000117]], b = [0.01000038]\n"
     ]
    }
   ],
   "source": [
    "#XOR Gate 학습\n",
    "xData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tData = np.array([0,1,1,0])\n",
    "\n",
    "xorGate = SimpleLogicGate(\"XOR Gate\",xData,tData)\n",
    "xorGate.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XOR Gate 학습 결과 검증 -> 올바른 예측 결과가 아님\n",
    "print(xorGate.gate,\"\\n\")\n",
    "testData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for x in testData:\n",
    "    (y , predicted) = xorGate.predict(x)\n",
    "    print(x, \" = \",predicted,\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input xData[0] = [0 0]\n",
      "[NAND output s1 = (array([1.]), 1)] [OR output s2 = (array([0.00999897]), 0)]\n",
      "s1_s2 = [1 0] ==> predected = 0\n",
      "xor_predected = [0]\n",
      "----------------------------------------\n",
      "Input xData[1] = [0 1]\n",
      "[NAND output s1 = (array([0.99334811]), 1)] [OR output s2 = (array([0.99000103]), 1)]\n",
      "s1_s2 = [1 1] ==> predected = 1\n",
      "xor_predected = [0, 1]\n",
      "----------------------------------------\n",
      "Input xData[2] = [1 0]\n",
      "[NAND output s1 = (array([0.99334811]), 1)] [OR output s2 = (array([0.99000103]), 1)]\n",
      "s1_s2 = [1 1] ==> predected = 1\n",
      "xor_predected = [0, 1, 1]\n",
      "----------------------------------------\n",
      "Input xData[3] = [1 1]\n",
      "[NAND output s1 = (array([8.75456087e-14]), 0)] [OR output s2 = (array([0.99999897]), 1)]\n",
      "s1_s2 = [0 1] ==> predected = 0\n",
      "xor_predected = [0, 1, 1, 0]\n",
      "----------------------------------------\n",
      "predicted result..........\n",
      "[0 0]  =  0\n",
      "[0 1]  =  1\n",
      "[1 0]  =  1\n",
      "[1 1]  =  0\n"
     ]
    }
   ],
   "source": [
    "#XOR Gate를 만들기 NAND, OR, AND 를 통해 만들수 있다.\n",
    "s1 = [] #NAND 의 출력\n",
    "s2 = [] # OR 의 출력\n",
    "s1_s2 = []\n",
    "xor_predected = []\n",
    "for index in range(len(xData)):\n",
    "    print(\"Input xData[{}] = {}\".format(index,xData[index]))\n",
    "    s1 = nandGate.predict(xData[index]) # NAND 출력\n",
    "    s2 = orGate.predict(xData[index]) #OR 출력\n",
    "    print(\"[NAND output s1 = {}] [OR output s2 = {}]\".format(s1,s2))\n",
    "    \n",
    "    s1_s2.append(s1[-1])\n",
    "    s1_s2.append(s2[-1])\n",
    "    (_,predicted) = andGate.predict(np.array(s1_s2))\n",
    "    print(\"s1_s2 = {} ==> predected = {}\".format(np.array(s1_s2),predicted))\n",
    "    \n",
    "    xor_predected.append(predicted)\n",
    "    print(\"xor_predected = {}\".format(xor_predected))\n",
    "    print(\"----------------------------------------\")\n",
    "    s1_s2 = []\n",
    "    \n",
    "print(\"predicted result..........\")\n",
    "for index in range(len(xData)):\n",
    "    print(xData[index], \" = \",xor_predected[index],end = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
